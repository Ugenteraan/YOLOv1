{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import xmltodict\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images_path     = 'VOCdevkit/VOC2012/JPEGImages'\n",
    "data_annotation_path = 'VOCdevkit/VOC2012/Annotations'\n",
    "image_height = 448\n",
    "image_width  = 448\n",
    "image_depth  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the naming of the image files and annotation files of VOC Pascal Dataset differs only in the extension,\n",
    "#sorting the lists would enable us to select an image file and its corresponding annotation file at the same \n",
    "#index position from the lists.\n",
    "list_images      = sorted([x for x in glob.glob(data_images_path + '/**')])     #length : 17125\n",
    "list_annotations = sorted([x for x in glob.glob(data_annotation_path + '/**')]) #length : 17125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VOCdevkit/VOC2012/JPEGImages/2007_005702.jpg', 'VOCdevkit/VOC2012/JPEGImages/2007_005705.jpg', 'VOCdevkit/VOC2012/JPEGImages/2007_005748.jpg', 'VOCdevkit/VOC2012/JPEGImages/2007_005759.jpg', 'VOCdevkit/VOC2012/JPEGImages/2007_005764.jpg']\n",
      "['VOCdevkit/VOC2012/Annotations/2007_005702.xml', 'VOCdevkit/VOC2012/Annotations/2007_005705.xml', 'VOCdevkit/VOC2012/Annotations/2007_005748.xml', 'VOCdevkit/VOC2012/Annotations/2007_005759.xml', 'VOCdevkit/VOC2012/Annotations/2007_005764.xml']\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "print(list_images[400:405])\n",
    "print(list_annotations[400:405])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_classes(xml_files=list_annotations):\n",
    "    '''Get all the classes in the dataset to construct one hot vector later.\n",
    "       Parameter\n",
    "       ---------\n",
    "       xml_files : a list containing paths to every single xml files.\n",
    "    '''\n",
    "    \n",
    "    classes = []\n",
    "    \n",
    "    for file in xml_files: #iterate through every xml files\n",
    "      \n",
    "        f = open(file)\n",
    "        doc = xmltodict.parse(f.read()) #parse the xml file\n",
    "        \n",
    "        #Some xml files may only contain one object tag as there's only 1 object in the image.\n",
    "        #For-looping over these tags throws a TypeError. Therefore, we use try-except to avoid this.\n",
    "        try:\n",
    "            for obj in doc['annotation']['object']: # try iterating through the objects in the xml file\n",
    "                    classes.append(obj['name'])\n",
    "        \n",
    "        except TypeError as e:\n",
    "            classes.append(doc['annotation']['object']['name'])\n",
    "        \n",
    "        f.close()\n",
    "            \n",
    "    classes = list(set(classes)) #set to remove duplicates.\n",
    "    classes.sort() #sort the list in ascending order\n",
    "    \n",
    "    \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n"
     ]
    }
   ],
   "source": [
    "classes = get_total_classes()\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = len(classes) #20\n",
    "S = 7 #cells\n",
    "B = 2 #num of bounding boxes per cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(xml_file_path):\n",
    "    '''Reads one file's annotation information and convert it to YOLO format.\n",
    "       Returns a label list for one image.\n",
    "       Parameter \n",
    "       ---------\n",
    "       xml_file_path : path to a Pascal VOC format xml file   | string\n",
    "    '''\n",
    "    \n",
    "    f   = open(xml_file_path)\n",
    "    doc = xmltodict.parse(f.read()) #parse the xml file and convert it into python dict\n",
    "    \n",
    "    height = doc['annotation']['size']['height']\n",
    "    width  = doc['annotation']['size']['width']\n",
    "    \n",
    "    #Each image must have labels for every cell. This means that in our case, S=7, C=20, we need to have\n",
    "    #[x,y,w,h,confidence, Pr(C_0),Pr(C_1), ... ,Pr(C_19)]. The length of the list would be 25. The confidence is\n",
    "    #zero when there is no object in the particular cell. Otherwise, the confidence is equal to the IoU between\n",
    "    #the predicted bounding box and the ground truth. Hence to calculate the confidence, in the fifth index of \n",
    "    #the list, we mark the Pr(object). If there is an object in the cell, Pr(object) = 1. 0 otherwise. When \n",
    "    #the network predicts the Pr(object), the predicted Pr(object) and the ground truth of Pr(object) will be \n",
    "    #used to calculate to calculate the confidence. NOTE that the length of the prediction is 30 since there are\n",
    "    #2 bounding box predictions. During training, only one of the box will be selected based on IoU. Hence,\n",
    "    #the label's list length is 25\n",
    "    label = [[0] * (5+C)] * S**2 #a 2-D list of zeros length 49 (S**2) where each element in the list is a list\n",
    "    #of zeros of length 25 (5 + C).\n",
    "    \n",
    "    \n",
    "    #Some xml files may only contain one object tag as there's only 1 object in the image.\n",
    "    #For-looping over these tags throws a TypeError. Therefore, we use try-except to avoid this.\n",
    "    try:\n",
    "        for obj in doc['annotation']['object']:#we have to iterate here since an img may contain more than 1 obj\n",
    "            \n",
    "            #retrieve the information from the xmldict\n",
    "            name  = obj['name']\n",
    "            x_min = obj['bndbox']['xmin']\n",
    "            x_max = obj['bndbox']['xmax']\n",
    "            y_min = obj['bndbox']['ymin']\n",
    "            y_max = obj['bndbox']['ymax']\n",
    "\n",
    "            #center of the box.\n",
    "            center_x = int(x_max) - int(x_min) \n",
    "            center_y = int(y_max) - int(y_min) \n",
    "\n",
    "            #the width and height of each cell when we divide the image into S x S cells.\n",
    "            cell_size_x = int(width)/S \n",
    "            cell_size_y = int(height)/S\n",
    "\n",
    "            '''\n",
    "            Quote from paper \n",
    "            ----------------\n",
    "            If the center of an object falls into a grid cell, that grid cell is responsible for detecting\n",
    "            that object.\n",
    "\n",
    "            '''\n",
    "            #get the cell that is responsible for the object and the value of the coordinates relative to \n",
    "            #the responsible grid cell.\n",
    "            x_coord_box, x_in_cell = divmod(center_x, cell_size_x)\n",
    "            y_coord_box, y_in_cell = divmod(center_y, cell_size_y)\n",
    "\n",
    "            #normalize the x and y coordinates in the cell.\n",
    "            x = x_in_cell/cell_size_x\n",
    "            y = y_in_cell/cell_size_y\n",
    "\n",
    "            #normalize the width and height of the bounding box relative to the entire image's width and height.\n",
    "            w = (int(x_max) - int(x_min))/int(width)\n",
    "            h = (int(y_max) - int(y_min))/int(height)\n",
    "            \n",
    "            #one-hot *list* for the class\n",
    "            one_hot_list = [0] * C #A list of zeros at length C\n",
    "            index = classes.index(name) #get the index of the class from the list 'classes'\n",
    "            one_hot_list[index] = 1.0 \n",
    "            \n",
    "            #list for each object. Round the floats to 2 decimal places\n",
    "            obj_info = [round(x,2),round(y,2),round(w,2),round(h,2), 1.0 ] + one_hot_list\n",
    "            \n",
    "            #since here we have the position of the box as a coordinate, we can convert that coordinate to box\n",
    "            #number with (x-coor + (y-coor x 7)). This is assuming the box numbering is from left to right\n",
    "            #starting from 0.\n",
    "            box_position = x_coord_box + (y_coord_box * 7)\n",
    "            label[int(box_position)] = obj_info #replace the list of zeros\n",
    "\n",
    "    #Some xml files may only contain one object tag as there's only 1 object in the image.\n",
    "    #For-looping over these tags throws a TypeError. Therefore, we use try-except to avoid this.\n",
    "    except TypeError as e:\n",
    "        \n",
    "        #Note that we use the doc dictionary, not obj dictionary\n",
    "        name  = doc['annotation']['object']['name']\n",
    "        x_min = doc['annotation']['object']['bndbox']['xmin']\n",
    "        x_max = doc['annotation']['object']['bndbox']['xmax']\n",
    "        y_min = doc['annotation']['object']['bndbox']['ymin']\n",
    "        y_max = doc['annotation']['object']['bndbox']['ymax']\n",
    "\n",
    "        #center of the box.\n",
    "        center_x = int(x_max) - int(x_min) \n",
    "        center_y = int(y_max) - int(y_min) \n",
    "\n",
    "        #the width and height of each cell when we divide the image into S x S cells.\n",
    "        cell_size_x = int(width)/S \n",
    "        cell_size_y = int(height)/S\n",
    "\n",
    "        '''\n",
    "        Quote from paper \n",
    "        ----------------\n",
    "        If the center of an object falls into a grid cell, that grid cell is responsible for detecting\n",
    "        that object.\n",
    "\n",
    "        '''\n",
    "        #get the cell that is responsible for the object and the value of the coordinates relative to \n",
    "        #the responsible grid cell.\n",
    "        x_coord_box, x_in_cell = divmod(center_x, cell_size_x)\n",
    "        y_coord_box, y_in_cell = divmod(center_y, cell_size_y)\n",
    "\n",
    "        #normalize the x and y coordinates in the cell.\n",
    "        x = x_in_cell/cell_size_x\n",
    "        y = y_in_cell/cell_size_y\n",
    "\n",
    "        #normalize the width and height of the bounding box relative to the entire image's width and height.\n",
    "        w = (int(x_max) - int(x_min))/int(width)\n",
    "        h = (int(y_max) - int(y_min))/int(height)\n",
    "        \n",
    "        #one-hot *list* for the class\n",
    "        one_hot_list = [0] * C #A list of zeros at length C\n",
    "        index = classes.index(name) #get the index of the class from the list 'classes'\n",
    "        one_hot_list[index] = 1.0 \n",
    "\n",
    "        #list for each object. Round the floats to 2 decimal places\n",
    "        obj_info = [round(x,2),round(y,2),round(w,2),round(h,2), 1.0 ] + one_hot_list\n",
    "        \n",
    "        #since here we have the position of the box as a coordinate, we can convert that coordinate to box\n",
    "        #number with (x-coor + (y-coor x 7)). This is assuming the box numbering is from left to right\n",
    "        #starting from 0.\n",
    "        box_position = x_coord_box + (y_coord_box * 7)\n",
    "        label[int(box_position)] = obj_info #replace the list of zeros\n",
    "    \n",
    "    f.close()\n",
    "        \n",
    "    return label #returns the label of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(first_index, last_index):\n",
    "    '''Load images into numpy array in a specific size (last_index - first_index).\n",
    "       Load annotations in YOLO format.\n",
    "       Returns np images and label\n",
    "       Parameter\n",
    "       ---------\n",
    "       first_index : integer\n",
    "       last_index  : integer\n",
    "    '''\n",
    "    \n",
    "    images = [] #initialize an empty list to append the images\n",
    "    labels    = [] #initialize an empty list to append the labels\n",
    "    \n",
    "    for i in range(first_index,last_index): \n",
    "        \n",
    "        im = cv2.imread(list_images[i])                 #read the images from the path\n",
    "        im = cv2.resize(im, (image_height,image_width)) #resize the images to 448x448x3\n",
    "        images.append(im)                               #append the image into the list\n",
    "        \n",
    "        label = get_label(list_annotations[i]) #get the list label for an image \n",
    "        labels.append(label) #append a single label into the list of labels\n",
    "        \n",
    "        \n",
    "    labels    = np.asarray(labels)    #convert the label list into np array\n",
    "    images    = np.asarray(images) #convert the images list into np array\n",
    "    \n",
    "    return (images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images,labels = load_dataset(100,110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(pred, truth):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X       = tf.placeholder(tf.float32, shape=(None, image_height, image_width, image_depth)) #(None, 448, 448, 3)\n",
    "Y       = tf.placeholder(tf.float32, shape=(None, S**2, 5+C)) #(None, 49, 25)\n",
    "dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "#output size : (None, 224, 224, 64)\n",
    "conv1 = tf.contrib.layers.conv2d(X, num_outputs=64, kernel_size=7, stride=2, \n",
    "                                 padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (None, 112, 112, 64)\n",
    "conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#output size : (None, 112, 112, 128)\n",
    "conv2 = tf.contrib.layers.conv2d(conv1_pool, num_outputs=128, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (None, 56, 56, 128)\n",
    "conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#output size : (None, 56, 56, 192)\n",
    "conv3 = tf.contrib.layers.conv2d(conv2_pool, num_outputs=192, kernel_size=1, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (None, 56, 56, 256)\n",
    "conv4 = tf.contrib.layers.conv2d(conv3, num_outputs=256, kernel_size=3, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (None, 56, 56, 256)\n",
    "conv5 = tf.contrib.layers.conv2d(conv4, num_outputs=256, kernel_size=1, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (None, 28, 28, 256)\n",
    "conv5_pool = tf.nn.max_pool(conv5, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#output size : (None, 28, 28, 512)\n",
    "conv6 = tf.contrib.layers.conv2d(conv5_pool, num_outputs=512, kernel_size=3, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (None, 28, 28, 512)\n",
    "conv7 = tf.contrib.layers.conv2d(conv6, num_outputs=512, kernel_size=1, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (None, 14, 14, 512)\n",
    "conv7_pool = tf.nn.max_pool(conv7, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#output size : (None, 14, 14, 600)\n",
    "conv8 = tf.contrib.layers.conv2d(conv7_pool, num_outputs=600, kernel_size=3, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (None, 7, 7, 600)\n",
    "conv8_pool = tf.nn.max_pool(conv8, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#output size : (None, 7, 7, 600)\n",
    "final_conv = tf.contrib.layers.conv2d(conv8_pool, num_outputs=600, kernel_size=3, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "output_shape = 7*7*600\n",
    "#feature vector shape : (None, 29400)\n",
    "feature_vector = tf.reshape(final_conv, (-1, 7*7*600))\n",
    "\n",
    "#Weight and bias variables for Fully connected layers\n",
    "W1 = tf.Variable(tf.truncated_normal([output_shape, 2048], stddev=0.1))\n",
    "B1 = tf.Variable(tf.constant(1.0, shape=[2048]))\n",
    "W2 = tf.Variable(tf.truncated_normal([2048, 7*7*30], stddev=0.1))\n",
    "B2 = tf.Variable(tf.constant(1.0, shape=[7*7*30]))\n",
    "\n",
    "#First fully-connected layer\n",
    "fc1 = tf.add(tf.matmul(feature_vector, W1), B1)\n",
    "fc1_actv = tf.nn.leaky_relu(fc1) #non-linear actv func\n",
    "\n",
    "#dropout\n",
    "dropout_layer = tf.nn.dropout(fc1_actv, dropout)\n",
    "\n",
    "#Second fully-connected layer\n",
    "fc2 = tf.add(tf.matmul(dropout_layer, W2), B2)\n",
    "\n",
    "Y_pred = tf.nn.sigmoid(fc2) #shape : [batch_size, 7*7*30]             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "\n",
    "#constants\n",
    "lambda_coord = 5\n",
    "lambda_noobj = 0.5\n",
    "\n",
    "prediction = tf.reshape(Y_pred, (-1, 49, 30))\n",
    "\n",
    "first_part_loss = lambda_coord * tf.reduce_sum(Y[:,:,4] * ((prediction[:,:,0] - Y[:,:,0])**2 +\n",
    "                                              (prediction[:,:,1] - Y[:,:,1])**2) +  )\n",
    "                                 \n",
    "\n",
    "second_part_loss = lambda_coord * tf.reduce_sum(Y[:,:,4] * ((tf.sqrt(prediction[:,:,2]) - tf.sqrt(Y[:,:,2]))**2 \n",
    "                                              + (tf.sqrt(prediction[:,:,3]) - tf.sqrt(Y[:,:,3])**2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "res1 = sess.run(conv1, feed_dict={X:images})\n",
    "res2 = sess.run(conv1_pool, feed_dict={X:images})\n",
    "res3 = sess.run(conv2, feed_dict={X:images})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa",
   "language": "python",
   "name": "nasa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
