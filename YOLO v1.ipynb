{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import xmltodict\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images_path     = 'VOCdevkit/VOC2012/JPEGImages'\n",
    "data_annotation_path = 'VOCdevkit/VOC2012/Annotations'\n",
    "image_height = 448\n",
    "image_width  = 448\n",
    "image_depth  = 3\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the naming of the image files and annotation files of VOC Pascal Dataset differs only in the extension,\n",
    "#sorting the lists would enable us to select an image file and its corresponding annotation file at the same \n",
    "#index position from the lists.\n",
    "list_images      = sorted([x for x in glob.glob(data_images_path + '/**')])     #length : 17125\n",
    "list_annotations = sorted([x for x in glob.glob(data_annotation_path + '/**')]) #length : 17125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VOCdevkit/VOC2012/JPEGImages/2007_005702.jpg', 'VOCdevkit/VOC2012/JPEGImages/2007_005705.jpg', 'VOCdevkit/VOC2012/JPEGImages/2007_005748.jpg', 'VOCdevkit/VOC2012/JPEGImages/2007_005759.jpg', 'VOCdevkit/VOC2012/JPEGImages/2007_005764.jpg']\n",
      "['VOCdevkit/VOC2012/Annotations/2007_005702.xml', 'VOCdevkit/VOC2012/Annotations/2007_005705.xml', 'VOCdevkit/VOC2012/Annotations/2007_005748.xml', 'VOCdevkit/VOC2012/Annotations/2007_005759.xml', 'VOCdevkit/VOC2012/Annotations/2007_005764.xml']\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "print(list_images[400:405])\n",
    "print(list_annotations[400:405])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_classes(xml_files=list_annotations):\n",
    "    '''Get all the classes in the dataset to construct one hot vector later.\n",
    "       Parameter\n",
    "       ---------\n",
    "       xml_files : a list containing paths to every single xml files.\n",
    "    '''\n",
    "    \n",
    "    classes = []\n",
    "    \n",
    "    for file in xml_files: #iterate through every xml files\n",
    "      \n",
    "        f = open(file)\n",
    "        doc = xmltodict.parse(f.read()) #parse the xml file\n",
    "        \n",
    "        #Some xml files may only contain one object tag as there's only 1 object in the image.\n",
    "        #For-looping over these tags throws a TypeError. Therefore, we use try-except to avoid this.\n",
    "        try:\n",
    "            for obj in doc['annotation']['object']: # try iterating through the objects in the xml file\n",
    "                    classes.append(obj['name'])\n",
    "        \n",
    "        except TypeError as e:\n",
    "            classes.append(doc['annotation']['object']['name'])\n",
    "        \n",
    "        f.close()\n",
    "            \n",
    "    classes = list(set(classes)) #set to remove duplicates.\n",
    "    classes.sort() #sort the list in ascending order\n",
    "    \n",
    "    \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n"
     ]
    }
   ],
   "source": [
    "classes = get_total_classes()\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = len(classes) #20\n",
    "S = 7 #cells\n",
    "B = 2 #num of bounding boxes per cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(xml_file_path):\n",
    "    '''Reads one file's annotation information and convert it to YOLO format.\n",
    "       Returns a label list for one image.\n",
    "       Parameter \n",
    "       ---------\n",
    "       xml_file_path : path to a Pascal VOC format xml file   | string\n",
    "    '''\n",
    "    \n",
    "    f   = open(xml_file_path)\n",
    "    doc = xmltodict.parse(f.read()) #parse the xml file and convert it into python dict\n",
    "    \n",
    "    height = doc['annotation']['size']['height']\n",
    "    width  = doc['annotation']['size']['width']\n",
    "    \n",
    "    #Each image must have labels for every cell. This means that in our case, S=7, C=20, we need to have\n",
    "    #[x,y,w,h,confidence, Pr(C_0),Pr(C_1), ... ,Pr(C_19)]. The length of the list would be 25. The confidence is\n",
    "    #zero when there is no object in the particular cell. Otherwise, the confidence is equal to the IoU between\n",
    "    #the predicted bounding box and the ground truth. Hence to calculate the confidence, in the fifth index of \n",
    "    #the list, we mark the Pr(object). If there is an object in the cell, Pr(object) = 1. 0 otherwise. When \n",
    "    #the network predicts the Pr(object), the predicted Pr(object) and the ground truth of Pr(object) will be \n",
    "    #used to calculate to calculate the confidence. NOTE that the length of the prediction is 30 since there are\n",
    "    #2 bounding box predictions. During training, only one of the box will be selected based on IoU. Hence,\n",
    "    #the label's list length is 25\n",
    "    label = [[0.0] * (5+C)] * S**2 #a 2-D list of zeros length 49 (S**2) where each element in the list is a list\n",
    "    #of zeros of length 25 (5 + C).\n",
    "    \n",
    "    \n",
    "    #Some xml files may only contain one object tag as there's only 1 object in the image.\n",
    "    #For-looping over these tags throws a TypeError. Therefore, we use try-except to avoid this.\n",
    "    try:\n",
    "        for obj in doc['annotation']['object']:#we have to iterate here since an img may contain more than 1 obj\n",
    "            \n",
    "            #retrieve the information from the xmldict\n",
    "            name  = obj['name']\n",
    "            x_min = obj['bndbox']['xmin']\n",
    "            x_max = obj['bndbox']['xmax']\n",
    "            y_min = obj['bndbox']['ymin']\n",
    "            y_max = obj['bndbox']['ymax']\n",
    "\n",
    "            #center of the box.\n",
    "            center_x = int(x_max) - int(x_min) \n",
    "            center_y = int(y_max) - int(y_min) \n",
    "\n",
    "            #the width and height of each cell when we divide the image into S x S cells.\n",
    "            cell_size_x = int(width)/S \n",
    "            cell_size_y = int(height)/S\n",
    "\n",
    "            '''\n",
    "            Quote from paper \n",
    "            ----------------\n",
    "            If the center of an object falls into a grid cell, that grid cell is responsible for detecting\n",
    "            that object.\n",
    "\n",
    "            '''\n",
    "            #get the cell that is responsible for the object and the value of the coordinates relative to \n",
    "            #the responsible grid cell.\n",
    "            x_coord_box, x_in_cell = divmod(center_x, cell_size_x)\n",
    "            y_coord_box, y_in_cell = divmod(center_y, cell_size_y)\n",
    "\n",
    "            #normalize the x and y coordinates in the cell.\n",
    "            x = x_in_cell/cell_size_x\n",
    "            y = y_in_cell/cell_size_y\n",
    "\n",
    "            #normalize the width and height of the bounding box relative to the entire image's width and height.\n",
    "            w = (int(x_max) - int(x_min))/int(width)\n",
    "            h = (int(y_max) - int(y_min))/int(height)\n",
    "            \n",
    "            #one-hot *list* for the class\n",
    "            one_hot_list = [0] * C #A list of zeros at length C\n",
    "            index = classes.index(name) #get the index of the class from the list 'classes'\n",
    "            one_hot_list[index] = 1.0 \n",
    "            \n",
    "            #list for each object. Round the floats to 2 decimal places\n",
    "            obj_info = [round(x,2),round(y,2),round(w,2),round(h,2), 1.0 ] + one_hot_list\n",
    "            \n",
    "            #since here we have the position of the box as a coordinate, we can convert that coordinate to box\n",
    "            #number with (x-coor + (y-coor x 7)). This is assuming the box numbering is from left to right\n",
    "            #starting from 0.\n",
    "            box_position = x_coord_box + (y_coord_box * 7)\n",
    "            label[int(box_position)] = obj_info #replace the list of zeros\n",
    "\n",
    "    #Some xml files may only contain one object tag as there's only 1 object in the image.\n",
    "    #For-looping over these tags throws a TypeError. Therefore, we use try-except to avoid this.\n",
    "    except TypeError as e:\n",
    "        \n",
    "        #Note that we use the doc dictionary, not obj dictionary\n",
    "        name  = doc['annotation']['object']['name']\n",
    "        x_min = doc['annotation']['object']['bndbox']['xmin']\n",
    "        x_max = doc['annotation']['object']['bndbox']['xmax']\n",
    "        y_min = doc['annotation']['object']['bndbox']['ymin']\n",
    "        y_max = doc['annotation']['object']['bndbox']['ymax']\n",
    "\n",
    "        #center of the box.\n",
    "        center_x = int(x_max) - int(x_min) \n",
    "        center_y = int(y_max) - int(y_min) \n",
    "\n",
    "        #the width and height of each cell when we divide the image into S x S cells.\n",
    "        cell_size_x = int(width)/S \n",
    "        cell_size_y = int(height)/S\n",
    "\n",
    "        '''\n",
    "        Quote from paper \n",
    "        ----------------\n",
    "        If the center of an object falls into a grid cell, that grid cell is responsible for detecting\n",
    "        that object.\n",
    "\n",
    "        '''\n",
    "        #get the cell that is responsible for the object and the value of the coordinates relative to \n",
    "        #the responsible grid cell.\n",
    "        x_coord_box, x_in_cell = divmod(center_x, cell_size_x)\n",
    "        y_coord_box, y_in_cell = divmod(center_y, cell_size_y)\n",
    "\n",
    "        #normalize the x and y coordinates in the cell.\n",
    "        x = x_in_cell/cell_size_x\n",
    "        y = y_in_cell/cell_size_y\n",
    "\n",
    "        #normalize the width and height of the bounding box relative to the entire image's width and height.\n",
    "        w = (int(x_max) - int(x_min))/int(width)\n",
    "        h = (int(y_max) - int(y_min))/int(height)\n",
    "        \n",
    "        #one-hot *list* for the class\n",
    "        one_hot_list = [0] * C #A list of zeros at length C\n",
    "        index = classes.index(name) #get the index of the class from the list 'classes'\n",
    "        one_hot_list[index] = 1.0 \n",
    "\n",
    "        #list for each object. Round the floats to 2 decimal places\n",
    "        obj_info = [round(x,2),round(y,2),round(w,2),round(h,2), 1.0 ] + one_hot_list\n",
    "        \n",
    "        #since here we have the position of the box as a coordinate, we can convert that coordinate to box\n",
    "        #number with (x-coor + (y-coor x 7)). This is assuming the box numbering is from left to right\n",
    "        #starting from 0.\n",
    "        box_position = x_coord_box + (y_coord_box * 7)\n",
    "        label[int(box_position)] = obj_info #replace the list of zeros\n",
    "    \n",
    "    f.close()\n",
    "        \n",
    "    return label #returns the label of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(first_index, last_index):\n",
    "    '''Load images into numpy array in a specific size (last_index - first_index).\n",
    "       Load annotations in YOLO format.\n",
    "       Returns np images and label\n",
    "       Parameter\n",
    "       ---------\n",
    "       first_index : integer\n",
    "       last_index  : integer\n",
    "    '''\n",
    "    \n",
    "    images = [] #initialize an empty list to append the images\n",
    "    labels    = [] #initialize an empty list to append the labels\n",
    "    \n",
    "    for i in range(first_index,last_index): \n",
    "        \n",
    "        im = cv2.imread(list_images[i])                 #read the images from the path\n",
    "        im = cv2.resize(im, (image_height,image_width)) #resize the images to 448x448x3\n",
    "        images.append(im)                               #append the image into the list\n",
    "        \n",
    "        label = get_label(list_annotations[i]) #get the list label for an image \n",
    "        labels.append(label) #append a single label into the list of labels\n",
    "        \n",
    "        \n",
    "    labels    = np.asarray(labels)    #convert the label list into np array\n",
    "    images    = np.asarray(images) #convert the images list into np array\n",
    "    \n",
    "    return (images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_dataset(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 49, 25)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_selector(box1, box2, truth):\n",
    "    '''Returns 3 tensors where the first two tensors are of values of either 1 or 0 based on which \n",
    "       bounding box should be selected and the third one is the IoU. The decision of which bounding box should \n",
    "       be selected is based on which predicted bounding box has the highest IoU with the ground truth.\n",
    "       Parameters\n",
    "       ----------\n",
    "       box1 : first predicted bounding box  | [batch_size, 49, 4]\n",
    "       box2 : second predicted bounding box | [batch_size, 49, 4]\n",
    "       truth: ground truth np array         | [batch_size, 49, 4]\n",
    "    '''\n",
    "    \n",
    "    #Since the ground truth labels and the predictions are normalized to the image size and the cell size, we\n",
    "    #will have to convert them back to coordinates in the image to calculate the IoU.\n",
    "    #Since the image has been resized to 448 x 488, we will use this as the height and width.\n",
    "    \n",
    "    #the width and height of each cell\n",
    "    cell_size_x = image_width/S\n",
    "    cell_size_y = image_height/S\n",
    "    \n",
    "    IoU_list = []\n",
    "    \n",
    "    #iterate through the cells\n",
    "    for i in range(S**2):\n",
    "        \n",
    "        '''\n",
    "        Ground truth coordinates\n",
    "        '''\n",
    "        \n",
    "        #y_offset tells us how many times we had to minus 7 from the current box position. It gives us\n",
    "        #the position of the box from the top while x_offset gives is the current box position from the left.\n",
    "        y_offset, x_offset = divmod(i,S)\n",
    "        \n",
    "        #iterates through each cell to get the tground ruth bounding box's normalized x,y,w and h values\n",
    "        truth_x_norm, truth_y_norm = truth[:,i,0], truth[:,i,1] #shape : [batch_size]\n",
    "        truth_w_norm, truth_h_norm = truth[:,i,2], truth[:,i,3] #shape : [batch_size]\n",
    "        \n",
    "        #to get the x coordinate in the image, we have to check on which cell the points fall into.\n",
    "        #If the point falls in the 5th cell from the left (ignore the y axis), then the x coordinate\n",
    "        #is equal to (4 x width of a cell) + (normalized_x_value x width of a cell). Hence the modulus to\n",
    "        #check how many cells from the left. Since the index starts from 0 we don't have to minus 1. As for y\n",
    "        #coordinate in the image, we have to check from the top box. Hence the y_offset.\n",
    "        truth_x = x_offset*cell_size_x + truth_x_norm*cell_size_x #shape : [batch_size]\n",
    "        truth_y = y_offset*cell_size_y + truth_y_norm*cell_size_y #shape : [batch_size]\n",
    "        \n",
    "        #since the width and height are normalized based on the entire image, to get the width and height\n",
    "        #we do : normalized width x image width = width. normalized height x image height = height.\n",
    "        truth_w = image_width * truth_w_norm  #shape : [batch_size]\n",
    "        truth_h = image_height * truth_h_norm #shape : [batch_size]\n",
    "        \n",
    "        #to calculate the IoU, it is easier to have the coordinate of the top-left and bottom-right of a box.\n",
    "        #ground-truth bounding box.\n",
    "       \n",
    "        #the coordinates are the center of the box. Therefore we use half of the width and height to find\n",
    "        #the box corner's coordinates.\n",
    "        truth_top_left_coor_x = truth_x - truth_w/2 #x coordinate of top left corner shape: [batch_size]\n",
    "        truth_top_left_coor_y = truth_y - truth_h/2 #y coordinate of top left corner shape: [batch_size]\n",
    "        truth_btm_rght_coor_x = truth_x + truth_w/2 #x coordinate of btm right corner shape: [batch_size]\n",
    "        truth_btm_rght_coor_y = truth_y + truth_h/2 #y coordinate of btm right corner shape: [batch_size]\n",
    "        \n",
    "        '''\n",
    "        bounding box 1 coordinate\n",
    "        '''\n",
    "        \n",
    "        box1_x_norm,box1_y_norm = box1[:,i,0], box1[:,i,1] \n",
    "        box1_w_norm,box1_h_norm = box1[:,i,2], box1[:,i,3]\n",
    "        \n",
    "        box1_x = x_offset*cell_size_x + box1_x_norm*cell_size_x \n",
    "        box1_y = y_offset*cell_size_y + box1_y_norm*cell_size_y \n",
    "        \n",
    "        box1_w = image_width  * box1_w_norm \n",
    "        box1_h = image_height * box1_h_norm \n",
    "        \n",
    "        box1_top_left_coor_x = box1_x - box1_w/2\n",
    "        box1_top_left_coor_y = box1_y - box1_h/2\n",
    "        box1_btm_rght_coor_x = box1_x + box1_w/2\n",
    "        box1_btm_rght_coor_y = box1_y + box1_h/2\n",
    "    \n",
    "        '''\n",
    "        bounding box 2 coordinate\n",
    "        '''\n",
    "        \n",
    "        box2_x_norm,box2_y_norm = box2[:,i,0], box2[:,i,1] \n",
    "        box2_w_norm,box2_h_norm = box2[:,i,2], box2[:,i,3]\n",
    "        \n",
    "        box2_x = x_offset*cell_size_x + box2_x_norm*cell_size_x \n",
    "        box2_y = y_offset*cell_size_y + box2_y_norm*cell_size_y \n",
    "        \n",
    "        box2_w = image_width  * box2_w_norm \n",
    "        box2_h = image_height * box2_h_norm \n",
    "        \n",
    "        box2_top_left_coor_x = box2_x - box1_w/2 \n",
    "        box2_top_left_coor_y = box2_y - box2_h/2\n",
    "        box2_btm_rght_coor_x = box2_x + box1_w/2\n",
    "        box2_btm_rght_coor_y = box2_y + box2_h/2\n",
    "        \n",
    "        '''\n",
    "        Calculate the IoU between each predicted box and the ground truth box\n",
    "        '''\n",
    "        \n",
    "        #ground truth box area. \n",
    "        #We need to add 1 because the coordinate starts from 0, hence the ending coordinate will always be 1 \n",
    "        #unit less than the image size. E.g. For an image of size 500 x 500. The starting coordinate is (0,0) \n",
    "        #while the ending coordinate is (499,499).\n",
    "        truth_box_area = (truth_btm_rght_coor_x - truth_top_left_coor_x + 1) * (truth_btm_rght_coor_y - \n",
    "                                                                                truth_top_left_coor_y + 1)\n",
    "        \n",
    "        '''\n",
    "        Box 1 and the ground truth\n",
    "        '''\n",
    "        #determine the x and y coordinates of the intersection rectangle\n",
    "        box1_x1 = tf.maximum(box1_top_left_coor_x, truth_top_left_coor_x)\n",
    "        box1_y1 = tf.maximum(box1_top_left_coor_y, truth_top_left_coor_y)\n",
    "        box1_x2 = tf.minimum(box1_btm_rght_coor_x, truth_btm_rght_coor_x)\n",
    "        box1_y2 = tf.minimum(box1_btm_rght_coor_y, truth_btm_rght_coor_y)\n",
    "        \n",
    "        #if the difference is less than 0, it means the boxes does not intersect\n",
    "        overlap_area = tf.maximum(0.0, box1_x2 - box1_x1) * tf.maximum(0.0, box1_y2 - box1_y1)\n",
    "        \n",
    "        #area of the first bounding box\n",
    "        box1_area = (box1_btm_rght_coor_x - box1_top_left_coor_x + 1) * (box1_btm_rght_coor_y - \n",
    "                                                                         box1_top_left_coor_y + 1)\n",
    "        \n",
    "        box1_iou = overlap_area/(box1_area + truth_box_area - overlap_area)\n",
    "        \n",
    "        '''\n",
    "        Box 2 and the ground truth\n",
    "        '''\n",
    "        #determine the x and y coordinates of the intersection rectangle\n",
    "        box2_x1 = tf.maximum(box2_top_left_coor_x, truth_top_left_coor_x)\n",
    "        box2_y1 = tf.maximum(box2_top_left_coor_y, truth_top_left_coor_y)\n",
    "        box2_x2 = tf.minimum(box2_btm_rght_coor_x, truth_btm_rght_coor_x)\n",
    "        box2_y2 = tf.minimum(box2_btm_rght_coor_y, truth_btm_rght_coor_y)\n",
    "        \n",
    "        #if the difference is less than 0, it means the boxes does not intersect\n",
    "        overlap_area = tf.maximum(0.0, box2_x2 - box2_x1) * tf.maximum(0.0, box2_y2 - box2_y1)\n",
    "        \n",
    "        #area of the first bounding box\n",
    "        box2_area = (box2_btm_rght_coor_x - box2_top_left_coor_x + 1) * (box2_btm_rght_coor_y - \n",
    "                                                                         box2_top_left_coor_y + 1)\n",
    "        \n",
    "        box2_iou = overlap_area/(box2_area + truth_box_area - overlap_area + 1e-9)\n",
    "        \n",
    "        IoU_list.append([box1_iou, box2_iou])\n",
    "    \n",
    "    #tensor of the IoU in shape of [batch_size, 49, 2]\n",
    "    IoU_array = tf.reshape(tf.convert_to_tensor(IoU_list), (-1, 49,2))\n",
    "    \n",
    "    \n",
    "    first_box_iou  = IoU_array[:,:,0] #shape : [batch_size, 49]\n",
    "    second_box_iou = IoU_array[:,:,1] #shape : [batch_size, 49]\n",
    "    \n",
    "    #returns a tensor of size [batch_size, 49] with 1.0 and 0.0 in its elements\n",
    "    #bb_select_box1 holds the truth values if box1's IoU is bigger than box2 or not. 1.0 for True ,0.0 for False\n",
    "    #bb_select_box2 holds the truth values if box2's IoU is bigger than box1 or not. 1.0 for True ,0.0 for False\n",
    "    bb_select_box1 = tf.cast(tf.greater(first_box_iou, second_box_iou), tf.float32)\n",
    "    bb_select_box2 = tf.cast(tf.greater(second_box_iou, first_box_iou), tf.float32)\n",
    "    \n",
    "    return (bb_select_box1, bb_select_box2, IoU_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X       = tf.placeholder(tf.float32, shape=(None, image_height, image_width, image_depth)) \n",
    "                                                                                    #(batch_size, 448, 448, 3)\n",
    "Y       = tf.placeholder(tf.float32, shape=(None, S**2, 5+C)) #(batch_size, 49, 25)\n",
    "dropout = tf.placeholder(tf.float32) #dropout rate\n",
    "\n",
    "#output size : (batch_size, 224, 224, 64)\n",
    "conv1 = tf.contrib.layers.conv2d(X, num_outputs=64, kernel_size=7, stride=2, \n",
    "                                 padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (batch_size, 112, 112, 64)\n",
    "conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#output size : (batch_size, 112, 112, 128)\n",
    "conv2 = tf.contrib.layers.conv2d(conv1_pool, num_outputs=128, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (batch_size, 56, 56, 128)\n",
    "conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#output size : (batch_size, 56, 56, 192)\n",
    "conv3 = tf.contrib.layers.conv2d(conv2_pool, num_outputs=192, kernel_size=1, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (batch_size, 56, 56, 256)\n",
    "conv4 = tf.contrib.layers.conv2d(conv3, num_outputs=256, kernel_size=3, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (batch_size, 56, 56, 256)\n",
    "conv5 = tf.contrib.layers.conv2d(conv4, num_outputs=256, kernel_size=1, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (batch_size, 28, 28, 256)\n",
    "conv5_pool = tf.nn.max_pool(conv5, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#output size : (batch_size, 28, 28, 512)\n",
    "conv6 = tf.contrib.layers.conv2d(conv5_pool, num_outputs=512, kernel_size=3, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (batch_size, 28, 28, 512)\n",
    "conv7 = tf.contrib.layers.conv2d(conv6, num_outputs=512, kernel_size=1, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (batch_size, 14, 14, 512)\n",
    "conv7_pool = tf.nn.max_pool(conv7, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#output size : (batch_size, 14, 14, 600)\n",
    "conv8 = tf.contrib.layers.conv2d(conv7_pool, num_outputs=600, kernel_size=3, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "#output size : (batch_size, 7, 7, 600)\n",
    "conv8_pool = tf.nn.max_pool(conv8, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#output size : (batch_size, 7, 7, 600)\n",
    "final_conv = tf.contrib.layers.conv2d(conv8_pool, num_outputs=600, kernel_size=3, stride=1,\n",
    "                                padding='SAME', activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "output_shape = 7*7*600\n",
    "#feature vector shape : (batch_size, 29400)\n",
    "feature_vector = tf.reshape(final_conv, (-1, 7*7*600))\n",
    "\n",
    "#Weight and bias variables for Fully connected layers\n",
    "W1 = tf.Variable(tf.truncated_normal([output_shape, 2048], stddev=1.0))\n",
    "B1 = tf.Variable(tf.constant(1.0, shape=[2048]))\n",
    "W2 = tf.Variable(tf.truncated_normal([2048, 7*7*30], stddev=1.0))\n",
    "B2 = tf.Variable(tf.constant(1.0, shape=[7*7*30]))\n",
    "\n",
    "#First fully-connected layer\n",
    "fc1 = tf.add(tf.matmul(feature_vector, W1), B1)\n",
    "fc1_actv = tf.nn.leaky_relu(fc1) #non-linear actv func\n",
    "\n",
    "#dropout\n",
    "dropout_layer = tf.nn.dropout(fc1_actv, dropout)\n",
    "\n",
    "#Second fully-connected layer\n",
    "fc2 = tf.add(tf.matmul(dropout_layer, W2), B2)\n",
    "\n",
    "Y_pred = tf.nn.sigmoid(fc2) #shape : [batch_size, 7*7*30]             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "\n",
    "#constants\n",
    "lambda_coord = 5\n",
    "lambda_noobj = 0.5\n",
    "epsilon = 1e-9\n",
    "\n",
    "prediction = tf.reshape(Y_pred, (-1, 49, 30))\n",
    "\n",
    "#input first bounding box, second bounding box and the ground truth bounding box\n",
    "box_selection1, box_selection2, iou = bbox_selector(prediction[:,:,0:4], prediction[:,:,5:9], Y[:,:,:4])\n",
    "\n",
    "#box_selection will ensure to pick the highest IoU predicted bounding box, while Y[:,:,4] at the beginning of\n",
    "#the term will ensure if there's any object at all in a particular cell.\n",
    "loss_1 = lambda_coord *(tf.reduce_sum(Y[:,:,4] * (box_selection1 *\n",
    "                                                 ((prediction[:,:,0] - Y[:,:,0])**2 + \n",
    "                                                  (prediction[:,:,1] - Y[:,:,1])**2) +\n",
    "                                                  box_selection2 * \n",
    "                                                  ((prediction[:,:,5] - Y[:,:,0])**2 +\n",
    "                                                   (prediction[:,:,6] - Y[:,:,1])**2))))\n",
    "\n",
    "\n",
    "loss_2 = lambda_coord *(tf.reduce_sum(Y[:,:,4] * (box_selection1 *\n",
    "                                                 ((tf.sqrt(prediction[:,:,2]+ epsilon) - tf.sqrt(Y[:,:,2]+ epsilon) )**2 + \n",
    "                                                  (tf.sqrt(prediction[:,:,3]+ epsilon)- tf.sqrt(Y[:,:,3]+ epsilon))**2) +\n",
    "                                                  box_selection2 * \n",
    "                                                  ((tf.sqrt(prediction[:,:,7]+ epsilon) - tf.sqrt(Y[:,:,2]+ epsilon))**2 +\n",
    "                                                   (tf.sqrt(prediction[:,:,8]+ epsilon) - tf.sqrt(Y[:,:,3]+ epsilon))**2))))\n",
    "\n",
    "#this part, I'm not sure if I understood it correctly\n",
    "loss_3 = tf.reduce_sum(Y[:,:,4] * (box_selection1 * \n",
    "                                   (prediction[:,:,4] * iou[:,:,0]) +\n",
    "                                    box_selection2* \n",
    "                                   (prediction[:,:,9] * iou[:,:,1]) -\n",
    "                                    (Y[:,:,4] *(box_selection1*iou[:,:,0] + box_selection2*iou[:,:,1])))**2)\n",
    "\n",
    "#change the 1.0 into 0.0 and 0.0 into 1.0 in Y[:,:,4].\n",
    "#cast the float to bool and back to float since logical_not module requires bool type data\n",
    "logical_not = tf.cast(tf.logical_not(tf.cast(Y[:,:,4], tf.bool)), tf.float32)\n",
    "\n",
    "loss_4 = lambda_noobj * (tf.reduce_sum(logical_not * (box_selection1 * \n",
    "                                   (prediction[:,:,4] * iou[:,:,0]) +\n",
    "                                    box_selection2* \n",
    "                                   (prediction[:,:,9] * iou[:,:,1]) -\n",
    "                                    (Y[:,:,4] *(box_selection1*iou[:,:,0] + box_selection2*iou[:,:,1])))**2))\n",
    "\n",
    "loss_5 = tf.reduce_sum(Y[:,:,4] * tf.reduce_sum((prediction[:,:,11:] - Y[:,:,6:])**2, axis=2))\n",
    "\n",
    "total_loss = loss_1 + loss_2+ loss_3 + loss_4 + loss_5\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "\n",
    "                            \n",
    "# second_part_loss = lambda_coord * tf.reduce_sum(Y[:,:,4] * ((tf.sqrt(prediction[:,:,2]) - tf.sqrt(Y[:,:,2]))**2 \n",
    "#                                               + (tf.sqrt(prediction[:,:,3]) - tf.sqrt(Y[:,:,3])**2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  184557.57147216797 Loss :  184557.57147216797\n",
      "Epoch :  175814.09999084473 Loss :  175814.09999084473\n",
      "Epoch :  174288.3465576172 Loss :  174288.3465576172\n",
      "Epoch :  167262.77444458008 Loss :  167262.77444458008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1953c9b0976f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Loss : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nasa/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nasa/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nasa/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nasa/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nasa/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nasa/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "epoch = 100\n",
    "\n",
    "for epoch_idx in range(epoch):\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(0,10000, batch_size):\n",
    "\n",
    "        images, labels = load_dataset(i, i+batch_size)\n",
    "\n",
    "        loss += sess.run([total_loss, optimizer], feed_dict={X:images, Y:labels, dropout:1.0})[0]\n",
    "        \n",
    "    print(\"Epoch : \", str(loss), \"Loss : \", str(loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa",
   "language": "python",
   "name": "nasa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
